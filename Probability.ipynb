{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A. MATHEMATICAL PRELIMINARIES\n",
    "\n",
    "# A.3. Probability Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability theory is a mathematically rigorous way of modeling\n",
    "uncertainty in the world. It should be noted that the probability values\n",
    "that are assigned by a human or autonomous system to various events may\n",
    "be subjective, based on faulty assumptions, estimated poorly, and\n",
    "otherwise incorrect. However, *probability theory describes\n",
    "mathematically sound ways to manipulate probabilities*, which are\n",
    "guaranteed to lead to accurate predictions as long as the base\n",
    "assumptions are accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Variables and the Joint Distribution\n",
    "\n",
    "A *random variable* $X$ has a domain $Val(X)$, and it is not known for\n",
    "certain what value $x \\in Val(X)$ it will take on. Instead, we define a\n",
    "*probability distribution* $P(X=x)$ that assigns nonnegative values to\n",
    "each $x \\in Val(X)$ describing the likelihood that $X$ will take on that\n",
    "value. As an example, the value showing when a die is rolled can be a\n",
    "random variable $X$ with $Val(X)=\\{1,2,3,4,5,6\\}$, and assuming the die\n",
    "is fair, $P(X=x)=1/6$ for all values of $x \\in Val(X)$. To make notation\n",
    "easier, we typically denote random variables in uppercase letters, and\n",
    "the corresponding value in lowercase.\n",
    "\n",
    "When the world contains multiple random variables $X_1,\\ldots,X_n$,\n",
    "their distribution is defined by the *joint distribution*\n",
    "$$P(X_1=x_1,\\ldots,X_n=x_n) \\equiv P((X_1=x_1) \\wedge \\cdots \\wedge (X_n=x_n)).$$\n",
    "In other words, the most basic events in the world are simultaneous\n",
    "settings of values to variables, and we are assigning probabilities to\n",
    "the *sample space*\n",
    "$S = \\{ (X_1=x_1) \\wedge \\cdots \\wedge(X_n=x_n) \\quad|\\quad x_1\\in Val(X_1),\\ldots,x_n\\in Val(X_n) \\}$.\n",
    "(Note that due to the symmetry of the $\\wedge$ operation, the order of\n",
    "arguments does not matter.)\n",
    "\n",
    "To be a valid probability distribution, $P$ must satisfy the axioms:\n",
    "\n",
    "-   $P(s) \\geq 0$ for all $s \\in S$\n",
    "\n",
    "-   $\\sum_{s \\in S} P(s) = 1$.\n",
    "\n",
    "The joint distribution is typically written as $P(X_1,\\ldots,X_n)$. This\n",
    "is a shorthand notation in which it is implicitly assumed to be a\n",
    "*function* over possible values assigned to these variables, and *not a\n",
    "single number*. We shall also refer to $P(x_1,\\ldots,x_n)$, *which is a\n",
    "single number* giving the probability $P(X_1=x_1,\\ldots,X_n=x_n)$.\n",
    "Moreover, in summations, we shall typically drop the domain of values\n",
    "that a variable can take on. If the notation is insufficiently clear, we\n",
    "shall explicitly write the assignments and domains.\n",
    "\n",
    "The joint probability distribution may be used to order to answer *any*\n",
    "probabilistic question that might be asked about variables in the world.\n",
    "Let $e$ be an *event* : some logical statement involving any subset of\n",
    "$X_1,\\ldots,X_N$ taking on certain values. Then $P(e)$ is defined as\n",
    "$$P(e) = \\sum_{s\\in S} P(s)I[e\\text{ holds in }s]$$ where $I[x]$ is the\n",
    "*indicator function* that is 1 if $x$ is true and 0 if $x$ is false.\n",
    "\n",
    "The joint probability distribution can also be manipulated to produce\n",
    "new distributions. As an example, we consider *marginalization*, which\n",
    "reduces the number of variables under consideration. If the world\n",
    "consists of random variables $X$ and $Y$, then the *marginal\n",
    "distribution* of $X$ is the function $P(X)$ specifying\n",
    "$$P(X=x) = \\sum_{y\\in Val(y)} P(X=x,Y=y).$$ In shorthand, we say:\n",
    "$$P(X) = \\sum_{y} P(X,y).$$ We also may say that $Y$ is *marginalized\n",
    "out* of the joint distribution.\n",
    "\n",
    "This operation can be performed multiple times to reduce the variable\n",
    "set. If the variables are $X$, $Y$, and $Z$, then marginalizing out $Z$\n",
    "produces a joint distribution over $X$ and $Y$ given by:\n",
    "$$P(X,Y) = \\sum_{z} P(X,Y,z),$$ specifying for each pair of values $x$\n",
    "and $y$ $$P(X=x,Y=y) = \\sum_{z \\in Val(Z)} P(X=x,Y=y,Z=z).$$ This can\n",
    "then be marginalized again to obtain a probability distribution over\n",
    "$X$.\n",
    "\n",
    "## Conditional Distributions and Independence\n",
    "\n",
    "The *conditional probability* of two events $e_1$ and $e_2$ is written\n",
    "as $P(e_1 | e_2)$ and is defined axiomatically as\n",
    "$$P(e_1 | e_2) = P(e_1 \\wedge e_2) / P(e_2).$$ Intuitively, it gives the\n",
    "probability that $e_1$ will hold after knowing that $e_2$ holds. Another\n",
    "convenient identity for conditional probabilities is\n",
    "$$P(e_1 \\wedge e_2) = P(e_1 | e_2) P(e_2).$$ More complex conditional\n",
    "rules include multiple events on the right hand side:\n",
    "$$P(e_1 | e_2 \\wedge e_3) = P(e_1 \\wedge e_2 \\wedge e_3) / P(e_2 \\wedge e_3)$$\n",
    "which can be easily shown to be equal to\n",
    "$$P(e_1 | e_2 \\wedge e_3) = P(e_1 \\wedge e_2 | e_3) / P(e_2 | e_3).\n",
    "\\label{eq:ConditioningThreeEvent}$$\n",
    "\n",
    "The *conditional distribution* expresses the probability distribution of\n",
    "one or more variables given the value of some other variable(s). If $Y$\n",
    "is known to take on the value $y$, then the conditional distribution of\n",
    "$X$ given $Y=y$ is $$P(X|y).$$ This is indeed a probability distribution\n",
    "over $X$ where the probability of $X=x$ is given by the conditional\n",
    "probability formula $$P(X=x|Y=y) = P(X=x,Y=y)/P(Y=y).$$ This can be\n",
    "thought of as a probability distribution $Q(X)$ over *restricted sample\n",
    "space* in which $Y=y$. In other words, the $Q$ is defined on the sample\n",
    "space $S_{|y} \\{ s \\in S \\quad|\\quad (Y=y) \\text{ holds in } s \\}$. $Q$\n",
    "then satisfies all of the probability axioms required of a probability\n",
    "distribution, as long as $S$ is replaced with $S_{|y}$.\n",
    "\n",
    "It is also possible to condition over multiple variables. The expression\n",
    "$P(X,Y|z,w)$ means the distribution over $X$ and $Y$ where the joint\n",
    "probability of any sample $X=x$, $Y=y$ is\n",
    "$$P(X=x,Y=y|Z=z,W=w) = P(X=x,Y=y,Z=z,W=w)/P(Z=z,W=w).$$ Conditioning\n",
    "rules for events also extends to variables, such as\n",
    "$$P(x,y,z|w) = P(x,y|z,w)P(z|w)$$ which is a restatement of\n",
    "($\\ref{eq:ConditioningThreeEvent}$).\n",
    "\n",
    "It is important to note that a conditional distribution is a true\n",
    "probability distribution over the variables on the left-hand side of the\n",
    "\"given\" mark. In particular the probability axioms are satisfied for the\n",
    "sample space of left-hand variables. Considering $P(X|y)$, we have\n",
    "$$\\sum_{x\\in Val(X)} P(x|y) = 1$$ and considering $P(X,Y|z,w)$,\n",
    "$$\\sum_{x\\in Val(X)} \\sum_{y\\in Val(Y)} P(x,y|z,w) = 1$$ On the other\n",
    "hand, $P(X,y|z)$ is *not* a probability distribution over $X$ (fixing\n",
    "the value of $Y$ at $y$ and $Z$ at $z$.)\n",
    "\n",
    "We may also refer to a conditional distribution $P(X|Y)$ *without*\n",
    "giving a specific value of $Y$. This provides the conditional\n",
    "distribution $P(X|y)$ for all values of $y$. Another way to think about\n",
    "$P(X|Y)$ is a two-argument function over values of $x$ and $y$ giving\n",
    "the value $P(X=x|Y=y)$. Such a distribution is referred to as *the\n",
    "probability of $X$ given $Y$*. If we were to write $P(X,Y|Z,W)$, this\n",
    "should be thought of as a four-argument function over values $x$, $y$,\n",
    "$z$, and $w$.\n",
    "\n",
    "Two events $e_1$ and $e_2$ are *independent* if $P(e_1 | e_2) = P(e_1)$.\n",
    "Equivalently, $P(e_1 \\wedge e_2) = P(e_1) P(e_2)$. Otherwise they are\n",
    "said to be *dependent*. Two random variables $X$ and $Y$ are said to be\n",
    "independent if the events $e_1 = (X=x)$ and $e_2 = (Y=y)$ are\n",
    "independent for all values $x\\in Val(X)$ and $y\\in Val(Y)$.\n",
    "\n",
    "*Bayes' rule* is another standard operation that can be used to change\n",
    "the order of conditioned statements. It is defined on events as\n",
    "$$P(e_1|e_2) = \\frac{P(e_2|e_1)P(e_1)}{P(e_2)}$$ and on random variables\n",
    "as $$P(X_1|X_2) = \\frac{P(X_2|X_1)P(X_1)}{P(X_2)}$$ in which this\n",
    "statement is taken to hold for all possible values that $X_1$ and $X_2$\n",
    "can take on. Both forms can be proven from elementary conditioning\n",
    "operations. In particular, Bayes' rule is useful because we can derive\n",
    "the conditional distribution $P(X|Y)$ knowing $P(Y|X)$ and $P(X)$. The\n",
    "distribution of $P(Y)$ can be derived using marginalization, and we can\n",
    "obtain\n",
    "$$P(X|Y) = \\frac{P(Y|X)P(X)}{P(Y)} = \\frac{P(Y|X)P(X)}{\\sum_{x\\in Val(X)} P(Y,x)} = \\frac{P(Y|X)P(X)}{\\sum_{x \\in Val(X)} P(Y|x)P(x)}.$$\n",
    "or in long-hand notation, $$\\begin{aligned}\n",
    "P(X=x|Y=y) &= \\frac{P(Y=y|X=x)P(X=x)}{P(Y=y)} = \\frac{P(Y=y|X=x)P(X=x)}{\\sum_{x^\\prime \\in Val(X)} P(Y=y,X=x^\\prime)} \\\\\n",
    "&= \\frac{P(Y=y|X=x)P(X=x)}{\\sum_{x^\\prime \\in Val(X)} P(Y=y|X=x^\\prime)P(X=x^\\prime)}.\n",
    "\\end{aligned}$$ (Note that the summation index in the denominator is\n",
    "*not* the same value of $x$ for which $P(X|Y)$ is being evaluated.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Distributions over Continuous Variables\n",
    "\n",
    "When considering probabilities over *continuous* random variables it\n",
    "becomes difficult to speak of a probability distribution, because the\n",
    "probability of taking on a single value is usually 0. As an example,\n",
    "suppose $X$ is a real random variable $Val(X) = \\mathbb{R}$ denoting a\n",
    "vehicle's true velocity, and $x \\in \\mathbb{R}$ is some value, such as a\n",
    "speedometer reading of 44 km/h. The probability axioms require that the\n",
    "sum of $P(X=x)$ over the infinite number of values $x \\in \\mathbb{R}$\n",
    "must equal 1, which means that $P(X=x)=0$ for almost all values of $x$.\n",
    "In other words, if we assigned a nonzero probability of the vehicle\n",
    "traveling at 44 km/h, then we would need to assign zero probability of\n",
    "traveling at 44.0001 km/h, or 44.0002 km/h, or 43.9999 km/h. Doing so\n",
    "would be somewhat strange, and would not accurately reflect our\n",
    "uncertainty of the true value of $X$, which should be a sort of\n",
    "continuously \"smeared\" probability distribution of values around\n",
    "44 km/h.\n",
    "\n",
    "So, when we speak of probability distributions over continuous\n",
    "variables, we usually refer to what is known as a *probability density\n",
    "function* (pdf). Pdfs share many properties of probability distributions\n",
    "(strictly considered) but are far more convenient to work with, because\n",
    "they do not collapse to assigning probability 0 everywhere.\n",
    "Specifically, a pdf $f$ for a random variable $X$ is defined as a\n",
    "nonnegative function $f:Val(X)\\rightarrow \\mathbb{R}$, $f(x) \\geq 0$\n",
    "such that: $$P(a \\leq X \\leq b) = \\int_a^b f(x) dx.$$ Where $P$ is a\n",
    "true probability distribution. As a result, a pdf must integrate to 1\n",
    "over the real number line:\n",
    "$$P(-\\infty \\leq X \\leq \\infty) = \\int_{-\\infty}^\\infty f(x) dx = 1.$$\n",
    "\n",
    "An alternative representation of continuous probability distributions is\n",
    "known as the *cumulative distribution function* (cdf). The cdf $F$\n",
    "corresponding to the pdf $f$ is a function defined as:\n",
    "$$F(c) \\equiv P(X \\leq c) = \\int_{-\\infty}^c f(x) dx.$$ Specifically,\n",
    "$F(c)$ is the probability that $X$ takes on a value less than or equal\n",
    "to $c$. Any cdf satisfies the following properties:\n",
    "\n",
    "-   $F(c) \\in [0,1]$ for all $c \\in \\mathbb{R}$.\n",
    "\n",
    "-   $F(-\\infty) = 0$, $F(\\infty) = 1$.\n",
    "\n",
    "-   $F^\\prime(c) = f(c)$.\n",
    "\n",
    "As a result of that last property, we see that $F^\\prime(c) \\geq 0$,\n",
    "which means that $F$ is *monotonically non-decreasing*. It is also\n",
    "straightforward to see that $P(a \\leq X \\leq b) = F(b) - F(a)$.\n",
    "\n",
    "Both pdfs and cdfs are in some sense equivalent representations of\n",
    "continuous probability distributions, but for computational and\n",
    "mathematical convenience we typically refer to the pdf form.\n",
    "Specifically, the pdf form is what is meant when referring to a\n",
    "probability $P(x)$.\n",
    "\n",
    "The most common distributions used in robotics are the uniform\n",
    "distribution and the Gaussian (aka normal) distribution.\n",
    "\n",
    "> The **uniform distribution** over the range $[a,b]$ prescribes an equivalent probability density\n",
    "> to each value $x$ in the range, and 0 everywhere else.\n",
    "\n",
    "In such a case we say $X \\sim U(a,b)$, and the pdf of this distribution is:\n",
    "$$f(x) = \\left\\{ \\begin{array}{ll} \\frac{1}{b-a} & \\text{if }a\\leq x \\leq b \\\\\n",
    "0 & \\text{otherwise} \\end{array}\\right.$$ We also denote this function\n",
    "as $U(x; a,b)$. The cdf of this distribution is\n",
    "$$F(x) = \\left\\{ \\begin{array}{ll} \\frac{x-a}{b-a} & \\text{if }a\\leq x \\leq b \\\\\n",
    "0 & \\text{if }a<x \\\\\n",
    "1 & \\text{if }b>x \\end{array}\\right.$$ Using the notation of the\n",
    "*Heaviside function* $H(x) = I[x \\geq 0]$, we can say more succinctly\n",
    "that $f(x) = \\frac{H(x-a)-H(x-b)}{b-a}$ and\n",
    "$F(x) = H(x-a)\\frac{x-a}{b-a} - H(x-b) \\frac{x-b}{b-a}$.\n",
    "\n",
    "We shall cover the Gaussian distribution below.\n",
    "\n",
    "## Multivariate continuous densities\n",
    "\n",
    "Multivariate continuous densities describe the joint distributions of\n",
    "multiple continuous variables. A multivariate cdf $F(x,y)$ gives\n",
    "$$P(X\\leq x,Y\\leq y) = F(x,y)$$ while the joint pdf $f(x,y)$ is defined\n",
    "such that\n",
    "$$F(a,b)  = \\int_{-\\infty}^{a}\\int_{-\\infty}^{b} f(x,y) dy dx.$$ An\n",
    "alternate definition of the density takes the limit\n",
    "$$\\lim_{\\epsilon\\rightarrow 0} P(x\\leq X\\leq x+ \\epsilon,y \\leq Y \\leq y+\\epsilon)/\\epsilon^2 = f(x,y).$$\n",
    "[Fig. 1](#fig:Gaussian) illustrates an example of the pdf and cdf for\n",
    "the bivariate Gaussian distribution.\n",
    "\n",
    "***********\n",
    "\n",
    "![fig:Gaussian](figures/appendix/multivariate_gaussian.svg)\n",
    "\n",
    "<div class=\"figcaption\"><a name=\"fig:Gaussian\">Figure 1.</a>\n",
    "    Left: A multivariate density function. Right: the cumulative\n",
    "distribution function corresponding to that\n",
    "density. </div>\n",
    "\n",
    "***********\n",
    "\n",
    "Marginalization can also be performed on continuous densities. If\n",
    "$f(x,y)$ is the joint density of random variables $X$ and $Y$, then\n",
    "$$g(x) = \\int_{-\\infty}^{\\infty} f(x,y) dy$$ is the density of $X$\n",
    "unconditional on $Y$. This operation is illustrated in\n",
    "[Fig. 2](#fig:GaussianMarginalization) (left).\n",
    "\n",
    "***********\n",
    "\n",
    "![fig:GaussianMarginalization](figures/appendix/multivariate_gaussian_marginalization.svg)\n",
    "\n",
    "<div class=\"figcaption\"><a name=\"fig:GaussianMarginalization\">Figure 2.</a>\n",
    "    Left: Marginalization of a multivariate density involves integration\n",
    "over a subset variable to obtain a distribution over the others. Right:\n",
    "Conditioning of a density involves \"slicing\" the distribution at a\n",
    "particular value and then\n",
    "re-normalizing.\n",
    "</div>\n",
    "\n",
    "***********\n",
    "\n",
    "\n",
    "We may also speak of the conditional probability density of a continuous\n",
    "variable. The standard definition $P(x|y) = P(x,y)/P(y)$ does not seem\n",
    "to apply, since $P(y)=0$ for essentially every value of $y$. However, it\n",
    "does work when interpreting probabilities as densities. Let $f(x,y)$ be\n",
    "the joint density and $g(y)$ be the density of $Y$. Let $F$ and $G$ be\n",
    "their respective cdfs. Let us then take the limit of the posterior cdf\n",
    "$P(X \\leq x | y \\leq Y \\leq y + \\epsilon)$ as $\\epsilon \\rightarrow 0$:\n",
    "$$P(X \\leq x | y \\leq Y \\leq y + \\epsilon) = \\frac{ \\int_{-\\infty}^{x} \\int_y^{y+\\epsilon} f(x^\\prime,y^\\prime) dy^\\prime dx^\\prime }{\\int_y^{y+\\epsilon} g(y^\\prime) dy^\\prime}$$\n",
    "As $\\epsilon \\rightarrow 0$, this becomes increasingly closer to\n",
    "$$P(X \\leq x | y \\leq Y \\leq y + \\epsilon) \\rightarrow \\frac{ \\int_{-\\infty}^{x} f(x^\\prime,y)\\epsilon dx^\\prime }{g(y) \\epsilon} = \\frac{ \\int_{-\\infty}^{x} f(x^\\prime,y) dx^\\prime }{g(y)} = P(X \\leq x | Y = y).$$\n",
    "Then, taking the derivative with respect to $x$, the conditional density\n",
    "becomes\n",
    "$$P(X=x | Y=y) = \\frac{d}{dx} P(X \\leq x | Y = y) = f(x,y)/g(y).$$ This\n",
    "can be thought of as taking a slice through the pdf with $Y=y$, and then\n",
    "normalizing, as illustrated in\n",
    "[Fig. 2](#fig:GaussianMarginalization) (right).\n",
    "\n",
    "## Mean, Standard Deviation, and Variance\n",
    "\n",
    "The mean, standard deviation, and variance are quantities that\n",
    "characterize the distribution of random variable. If $X \\sim P(X)$ is a\n",
    "continuous random variable, then the *mean* is defined as\n",
    "$$\\bar{X} = \\int_{-\\infty}^{\\infty} x P(x) dx.$$ This is also known as\n",
    "the *expected value* of the distribution, which is denoted $E[X]$.\n",
    "\n",
    "The *variance* of the distribution is the expected value of the squared\n",
    "difference between the variable's value and the mean, and gives a sense\n",
    "of the spread of the distribution:\n",
    "$$Var[X] = E[(X - \\bar{X})^2] = \\int_{-\\infty}^{\\infty} (x - \\bar{X})^2 P(x) dx.$$\n",
    "Variance is always nonnegative, and is only 0 if $X$ has a nonzero\n",
    "probability of taking on a value other than $\\bar{X}$. The *standard\n",
    "deviation* is simply the square root of the variance,\n",
    "$Std[X] = \\sqrt{Var[X]}$.\n",
    "\n",
    "Given two variables $X$ and $Y$, their *covariance* is defined as\n",
    "$$Cov[X,Y] = E[(X - \\bar{X})(Y - \\bar{Y})].$$ The covariance of two\n",
    "independent variables is 0. Covariance is positive if knowing that $X$\n",
    "is larger than average provides information that $Y$ is larger than\n",
    "average (positive correlation), and it is negative if it provides\n",
    "information that $Y$ is smaller than average (negative correlation). It\n",
    "is also apparent that $Cov[X,X]=Var[X]$.\n",
    "\n",
    "If we wish to specify a probability distribution over which an expected\n",
    "value, variance, or covariance should be evaluated, it can be written in\n",
    "the subscript. For example, the expected value of $X$ knowing that $Y=y$\n",
    "can be written as:\n",
    "$$E_{P(X|y)}[X] = \\int_{-\\infty}^{\\infty} x P(x|y) dx.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Operations on Random Variables\n",
    "\n",
    "It is not so easy to perform operations and transformations on random\n",
    "variables. Adding two random variables is not as simple as adding their\n",
    "pdfs, and even applying simple functions is not straightforward.\n",
    "\n",
    "Suppose we wish to compute the distribution of a deterministic function\n",
    "$h(x)$ with $X$ a random variable. To answer this question, we consider\n",
    "the event space of two continuous random variables $X$ and $Y$ *related*\n",
    "by the constraint $y=h(x)$. For example, suppose that it is known that\n",
    "$y = x^2$, but we only have probabilistic knowledge about the value of\n",
    "$x$. What is the probability distribution over $Y$ that is consistent\n",
    "with this information?\n",
    "\n",
    "If we know that $f$ and $F$ are the pdf and cdf of $X$, respectively,\n",
    "*and* we know that $h$ is monotonically increasing, then we can\n",
    "determine the pdf of $Y$ using the *probability integral transform*. Let\n",
    "$g$ and $G$ be the unknown pdf and cdf of $Y$, and let us examine how to\n",
    "compute a specific value of $G(y)$. Each point $x$ \"counts\" toward the\n",
    "sum as long as $h(x) \\leq y$. Hence, the following equation holds:\n",
    "$$G(y) = P(Y \\leq y) = \\int_{-\\infty}^{\\infty} I[h(x) \\leq y] P(X=x) dx.\n",
    "\\label{eq:ProbabilityTransformedY}$$ If we use the assumption that $h$\n",
    "is monotonically increasing, then $h(x) \\leq y$ is true if and only if\n",
    "$x \\leq h^{-1}(y)$. Hence, we can rewrite\n",
    "($\\ref{eq:ProbabilityTransformedY}$) as\n",
    "$$G(y) = \\int_{-\\infty}^{h^{-1}(y)} f(x) dx = F(h^{-1}(y)).\n",
    "\\label{eq:ProbabilityTransformedY2}$$ Using this formula, we can derive\n",
    "a few results about simple transformations:\n",
    "\n",
    "-   If $c$ is a constant, then $P(X + c \\leq x) = P(X \\leq x-c)$.\n",
    "\n",
    "-   $P(-X\\leq x) = P(X \\geq -x)$.\n",
    "\n",
    "-   If $a > 0$ is a constant, then $P(aX+b \\leq x) = P(X \\leq (x-b)/a)$.\n",
    "\n",
    "-   If $a < 0$, then $P(aX+b \\leq x) = P(X \\geq (x-b)/a)$.\n",
    "\n",
    "Let us now return to the function $h(x)=x^2$ as we proposed originally.\n",
    "This is not monotonically increasing, so we cannot directly apply\n",
    "($\\ref{eq:ProbabilityTransformedY2}$). However, we can split its\n",
    "domain into two parts, $x \\geq 0$ and $x < 0$. The second part is\n",
    "monotonically decreasing, so we shall have to handle that slightly\n",
    "differently. We can still apply\n",
    "($\\ref{eq:ProbabilityTransformedY}$) to obtain\n",
    "$$G(y) = P(Y \\leq y) = \\int_{-\\infty}^{\\infty} I[x^2 \\leq y] f(x) dx$$\n",
    "which can be split into two parts in which $h$ is monotonic. Performing\n",
    "a flip in the integration range for the negatively decreasing part, we\n",
    "obtain: $$\\begin{aligned}\n",
    "G(y) &= \\int_{-\\infty}^{0} I[x^2 \\leq y] f(x) dx + \\int_{0}^{\\infty} I[x^2 \\leq y] f(x) dx \\\\\n",
    "& = \\int_{-\\sqrt y}^{0} f(x) dx + \\int_{0}^{\\sqrt y} f(x) dx \\\\\n",
    "& = (F(0) - F(-\\sqrt y)) + (F(\\sqrt y) - F(0)) = F(\\sqrt y)-F(-\\sqrt y).\n",
    "\\end{aligned}$$\n",
    "\n",
    "If we know that a random variable $Z$ is the sum of two random variables\n",
    "$X$ and $Y$, we must consider *all* the ways that the sum $z=x+y$ can be\n",
    "formed. Via marginalization,\n",
    "$$F(z) = P(Z\\leq z) = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} I[x+y\\leq z] P(X=x,Y=y) dx dy.$$\n",
    "If $X$ and $Y$ are independent, then we can simplify the double integral\n",
    "into a single integral over either $X$ or $Y$: $$\\begin{aligned}\n",
    "F(z) = P(Z\\leq z) &= \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} I[x+y\\leq z] P(X=x)P(Y=y) dx dy \\\\\n",
    "&= \\int_{-\\infty}^{\\infty} P(X\\leq z-y)P(Y=y) dy %\\\\\n",
    "%&= \\int_{-\\infty}^{\\infty} P(Y\\leq z-x)P(X=x) dx.\n",
    "\\end{aligned}$$ Another way to express this is with the pdf, which is\n",
    "the derivative of the cdf: $$\\begin{aligned}\n",
    "f(z) = \\frac{d F}{d z}(z) \n",
    "&= \\int_{-\\infty}^{\\infty} P(X=z-y)P(Y=y) dy %\\\\\n",
    "%&= \\int_{-\\infty}^{\\infty} P(Y=z-x)P(X=x) dx.\n",
    "\\end{aligned}$$\n",
    "\n",
    "This is not so easy to compute in general; for example, the sum of two\n",
    "uniformly distributed random variables $X$ and $Y$ on the range $[0,1]$\n",
    "has the pdf $P(X+Y = z) = 0$ for $z \\leq 0$ and $z \\geq 0$, and for all\n",
    "other $z$ $$\\begin{aligned}\n",
    "P(X+Y \\leq z) &= \\int_{-\\infty}^{\\infty} P(X=z-y)P(Y=y) dy \\\\\n",
    "& = \\int_{0}^{1} P(X = z-y) dy \\\\\n",
    "& = \\int_{0}^{1} (I[z\\geq y]-I[z\\geq y+1]) dy \\\\\n",
    "& = \\int_{0}^{1} I[z\\geq y]dy - \\int_{0}^{1} I[z\\geq y+1] dy.\n",
    "\\end{aligned}$$ If $z \\geq 1$, then $z \\geq y$ and the indicator\n",
    "function in the first integral is always active. If $z < 1$, then\n",
    "$z < y+1$ and the indicator function in the second integral is never\n",
    "active. Hence, if $z < 1$,\n",
    "$$P(X+Y = z) = \\int_0^1 I[z \\geq y]dy = \\int_0^z 1 dy = z$$ and for\n",
    "$z \\geq 1$, $$\\begin{aligned}\n",
    "P(X+Y = z) &= \\int_0^1 I[z \\geq y]dy - \\int_0^1 I[z \\geq y+1] \\\\\n",
    "& = \\int_0^1 1 dy - \\int_0^{z-1} 1 dy = 1 - (z-1) = 2-z.\n",
    "\\end{aligned}$$\n",
    "\n",
    "Fortunately, simple, closed form solutions exist for certain operations\n",
    "and classes of distributions. One of the most important classes of distribution is the Gaussian distribution, which is closed under linear transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gaussian distribution\n",
    "\n",
    "### Univariate Gaussians\n",
    "\n",
    "The univariate Gaussian (or normal) distribution with mean $\\mu$ and standard\n",
    "deviation $\\sigma$ has the pdf:\n",
    "$$P(x) \\equiv N(x;\\mu,\\sigma^2) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n",
    "This function is has the shape of the famous bell curve, and has a peak\n",
    "at $x=\\mu$ and has spread controlled by $\\sigma$. We say a Gaussian (or\n",
    "normal) variable $X$ has distribution $\\mathcal{N}(\\mu,\\sigma)$, or\n",
    "$X \\sim N(\\mu,\\sigma^2)$.\n",
    "\n",
    "The significance of this distribution is the *central limit theorem* :\n",
    "the distribution of the mean of a sample of independent, identically\n",
    "distributed random variables approaches a Gaussian as the sample size\n",
    "grows larger. This holds under very few assumptions about the\n",
    "distribution of each variable. Specifically, given\n",
    "$X_1,\\ldots,X_n \\sim P(X)$, the sample mean $M_n$ is defined as\n",
    "$$M_n = \\frac{1}{n}\\sum_{i=1}^n X_n.$$ Roughly, the central limit\n",
    "theorem states that $M_n$ converges to a gaussian distribution:\n",
    "$$P(M_n = x) \\approx N(x;\\bar{X_i},Var[X_i]/n) \\text{ as }n\\rightarrow \\infty.$$\n",
    "\n",
    "It can also be shown that a linear transformation of a Gaussian variable\n",
    "is also Gaussian. Specifically, with $X \\sim N(\\mu,\\sigma^2)$, we have\n",
    "$$aX+b \\sim N(a\\mu+b,a^2 \\sigma^2).$$\n",
    "\n",
    "The sum of two independent Gaussian variables is also Gaussian. If\n",
    "$X \\sim N(\\mu_X,\\sigma_X^2)$, $Y \\sim N(\\mu_Y,\\sigma_Y^2)$ are\n",
    "independent, then $$X + Y \\sim N(\\mu_X+\\mu_Y,\\sigma_X^2+\\sigma_Y^2)$$\n",
    "\n",
    "### Multivariate Gaussians\n",
    "\n",
    "A multivariate Gaussian distribution over a vector-valued random\n",
    "variable $\\textbf{X}=[X_1,...,X_n]^T \\in \\mathbb{R}^n$ with mean vector\n",
    "$\\boldsymbol{\\mu}$ and covariance matrix $\\Sigma$ has the density\n",
    "function:\n",
    "$$P(\\textbf{x}) = N(\\textbf{x};\\boldsymbol{\\mu},\\Sigma) = \\frac{1}{(2\\pi)^{n/2}\\sqrt{|\\Sigma|}} e^{-\\frac{1}{2} (\\textbf{x}-\\boldsymbol{\\mu})^T \\Sigma^{-1} (\\textbf{x}-\\boldsymbol{\\mu})}.\n",
    "\\label{eq:MultivariateGaussian}$$ Like the (univariate) Gaussian\n",
    "distribution, it has a peak at $x=\\boldsymbol{\\mu}$, and its spread is\n",
    "determined by the matrix $\\Sigma$.\n",
    "[Fig. 1](#fig:Gaussian) shows the pdf of a bivariate Gaussian with\n",
    "$\\boldsymbol{\\mu}=0$ and $\\Sigma=I$.\n",
    "\n",
    "The *covariance matrix* is defined as $$\\Sigma_{ij} = Cov[X_i,X_j].$$\n",
    "$\\Sigma$ must be symmetric positive definite as well. Its diagonal\n",
    "entries define the variance of individual elements of $\\textbf{x}$, while the\n",
    "off-diagonals determine how much it is skewed.\n",
    "\n",
    "For example, consider a 2D Gaussian with mean\n",
    "$\\boldsymbol{\\mu} = \\begin{bmatrix}\\mu_1 \\\\ \\mu_2 \\end{bmatrix}$ and covariance\n",
    "$$\\Sigma = \\begin{bmatrix}\\Sigma_1 & \\Sigma_{12} \\\\ \\Sigma_{12} & \\Sigma_{2} \\end{bmatrix}.$$ If\n",
    "$\\Sigma_{12} = 0$, then $X_1$ and $X_2$ are independent, with respective\n",
    "distributions $X_1 \\sim N(\\mu_1,\\Sigma_1)$ and\n",
    "$X_2 \\sim N(\\mu_2,\\Sigma_2)$. Otherwise, the marginal distributions stay\n",
    "the same: $$\\begin{aligned}\n",
    "P(X_1=x) &= N(x;\\mu_1,\\Sigma_1) \\\\\n",
    "P(X_2=x) &= N(x;\\mu_2,\\Sigma_2)\\end{aligned}$$ but the joint\n",
    "distribution is skewed. If $\\Sigma_{12} > 0$, it is skewed in the upper\n",
    "right and lower left quadrants, while if $\\Sigma_{12} < 0$, it is skewed\n",
    "in the upper left and lower right quadrants.\n",
    "\n",
    "Deriving the following facts requires some extensive manipulation\n",
    "of ($\\ref{eq:MultivariateGaussian}$). We shall simply state them without\n",
    "proof; the proofs are left as exercise for the interested reader.\n",
    "\n",
    "#### Stacking of multivariate Gaussians\n",
    "Independent multivariate Gaussian variables can be stacked according to\n",
    "the following rule. If $\\textbf{X} \\sim N(\\boldsymbol{\\mu}_X,\\Sigma_X)$ and\n",
    "$\\textbf{Y} \\sim N(\\boldsymbol{\\mu}_Y,\\Sigma_Y)$ are independent, then\n",
    "$$\\textbf{Z} = \\begin{bmatrix}\\textbf{X} \\\\ \\textbf{Y} \\end{bmatrix} \\sim N\\left(\\begin{bmatrix}\\boldsymbol{\\mu}_X \\\\ \\boldsymbol{\\mu}_Y \\end{bmatrix}, \\begin{bmatrix}\\Sigma_X & 0 \\\\ 0 & \\Sigma_Y \\end{bmatrix}\\right)$$\n",
    "where the mean and covariance matrix are written in block-matrix form.\n",
    "\n",
    "#### Marginalization of multivariate Gaussians\n",
    "If $\\textbf{Z} = \\begin{bmatrix}\\textbf{X} \\\\ \\textbf{Y} \\end{bmatrix}$ is Gaussian:\n",
    "$$\\textbf{Z} = \\begin{bmatrix}\\textbf{X} \\\\ \\textbf{Y} \\end{bmatrix} \\sim N\\left( \\begin{bmatrix}\\boldsymbol{\\mu}_X \\\\ \\boldsymbol{\\mu}_Y \\end{bmatrix}, \\begin{bmatrix} \\Sigma_X & \\Sigma_{XY} \\\\ \\Sigma_{YX} & \\Sigma_Y\\end{bmatrix} \\right)$$\n",
    "then the marginal distribution of $\\textbf{X}$ is\n",
    "$\\textbf{X} \\sim N(\\boldsymbol{\\mu}_X,\\Sigma_X)$, and the marginal\n",
    "distribution of $\\textbf{y}$ is $\\textbf{Y} \\sim N(\\boldsymbol{\\mu}_Y,\\Sigma_Y)$.\n",
    "\n",
    "\n",
    "![fig:GaussianTransform](figures/appendix/gaussian_linear_transform.svg)\n",
    "\n",
    "<div class=\"figcaption\"><a name=\"fig:GaussianTransform\">Figure 3</a>.\n",
    "    An affine transform of a Gaussian distribution is also a Gaussian. A\n",
    "transform $A\\textbf{x}+\\textbf{b}$ (left) is applied to an initial zero-mean\n",
    "distribution, whose density is displayed in the leftmost contour plot.\n",
    "The middle contour plot displays the density of the resulting\n",
    "distribution. Applying the transform again yields a density in the\n",
    "rightmost contour\n",
    "plot.\n",
    "    </div>\n",
    "\n",
    "Affine transformations of multivariate Gaussians are also Gaussian\n",
    "([Fig. 3](#fig:GaussianTransform)). If\n",
    "$\\textbf{X} \\sim N(\\boldsymbol{\\mu},\\Sigma)$, then for any appropriately\n",
    "sized matrix $A$ and vector $\\textbf{b}$, $$A\\textbf{X}+\\textbf{b} \\sim N(A\\boldsymbol{\\mu}+\\textbf{b},A\\Sigma A^T).$$ \n",
    "More specifically, $\\textbf{X}$ and $A\\textbf{X}+\\textbf{b}$ are jointly distributed\n",
    "according to:\n",
    "$$\\begin{bmatrix}\\textbf{X} \\\\ A\\textbf{X}+\\textbf{b} \\end{bmatrix} \\sim N\\left(\\begin{bmatrix}\\boldsymbol{\\mu} \\\\ A\\boldsymbol{\\mu}+\\textbf{b} \\end{bmatrix},\\begin{bmatrix}\\Sigma & \\Sigma A^T \\\\ A\\Sigma & A\\Sigma A^T\\end{bmatrix}\\right).$$\n",
    "\n",
    "#### Sum of multivariate Gaussians\n",
    "The sum of independent multivariate Gaussians is also Gaussian, which\n",
    "can be derived by stacking and linear transformation rule. If\n",
    "$\\textbf{X} \\sim N(\\boldsymbol{\\mu}_X,\\Sigma_X)$ and\n",
    "$\\textbf{Y} \\sim N(\\boldsymbol{\\mu}_Y,\\Sigma_Y)$ are independent $n$-D\n",
    "Gaussian variables, then\n",
    "$$\\textbf{Z} = \\textbf{X}+\\textbf{Y} = [I_n \\quad I_n]\\begin{bmatrix}\\textbf{X} \\\\ \\textbf{Y} \\end{bmatrix} = N(\\boldsymbol{\\mu}_X+\\boldsymbol{\\mu}_Y,\\Sigma_X+\\Sigma_Y)$$\n",
    "where $[I_n \\quad I_n]$ is the horizontal block matrix of two identity\n",
    "matrices.\n",
    "\n",
    "#### Conditioning of multivariate Gaussians\n",
    "Conditional distributions of multivariate Gaussians are also Gaussian.\n",
    "Specifically, if $Val(\\textbf{X}) = \\mathbb{R}^n$ and\n",
    "$Val(\\textbf{Y}) = \\mathbb{R}^m$ are jointly Gaussian, that is,\n",
    "$$\\begin{bmatrix}{\\textbf{X}} \\\\ {\\textbf{Y}} \\end{bmatrix} \\sim N\\left(\\begin{bmatrix}{\\boldsymbol{\\mu}_X} \\\\ {\\boldsymbol{\\mu}_Y} \\end{bmatrix},\\begin{bmatrix}{\\Sigma_X}&{\\Sigma_{XY}} \\\\ {\\Sigma_{XY}^T}&{\\Sigma_Y} \\end{bmatrix}\\right),$$\n",
    "then the conditional distribution of $\\textbf{X}$ given that $\\textbf{Y}=\\textbf{y}$ is\n",
    "also Gaussian:\n",
    "$$P(\\textbf{x}|\\textbf{y}) = N\\left( \\textbf{x};\\boldsymbol{\\mu}_X+\\Sigma_{XY}\\Sigma_Y^{-1}(\\textbf{y}-\\boldsymbol{\\mu}_Y),\\Sigma_X - \\Sigma_{XY}\\Sigma_Y^{-1}\\Sigma_{XY}^T \\right)\n",
    "\\label{eq:GaussianConditioning}$$ It can be observed that the mean of\n",
    "the new distribution depends on the value of $y$, but the covariance\n",
    "does not. Observe also that if $\\textbf{X}$ and $\\textbf{Y}$ are independent\n",
    "($\\Sigma_{XY}=0$), then knowing the value of $\\textbf{Y}$ does not change the\n",
    "marginal distribution of $\\textbf{X}$.\n",
    "\n",
    "[Fig. 2](#fig:GaussianMarginalization), right illustrates conditioning\n",
    "on a zero-mean Gaussian with $$\\Sigma=\\begin{bmatrix}{0.25}&{0.3} \\\\ {0.3}&{1} \\end{bmatrix}$$ at\n",
    "$x_1=1$. According to\n",
    "($\\ref{eq:GaussianConditioning}$), the mean of $x_2$ becomes\n",
    "$0.3\\cdot 0.25^{-1} \\cdot 1 = 1.2$, and its variance becomes\n",
    "$1 - 0.3\\cdot 0.25^{-1} \\cdot 0.3 = 0.64$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": true,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section IV. DYNAMICS AND CONTROL\n",
    "    \n",
    "# Chapter 17. Optimal Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous chapters we have seen the use of myopic controllers like PID\n",
    "or operational space control, as well as some predictive controllers\n",
    "like trajectory generation. Particularly for complex and nonlinear\n",
    "systems like robots, predictive control allows the controller to make\n",
    "better decisions at the current time to account for future\n",
    "possibilities. However, our previous predictive methods were largely\n",
    "restricted to one class of systems. Optimal control addresses these\n",
    "shortcomings in a highly general framework.\n",
    "\n",
    "Optimal control asks to compute a control function (either open loop or\n",
    "closed loop) that optimizes some performance metric regarding the\n",
    "control and the predicted state. For example, a driver of a car would\n",
    "like to reach a desired location while achieving several other goals:\n",
    "e.g., avoiding obstacles, not driving erratically, maintaining a\n",
    "comfortable level of accelerations for human passengers. A driving style\n",
    "can be composed of some balanced combination of these goals. Optimal\n",
    "control allows a control designer to specify the *dynamic model* and the\n",
    "*desired outcomes*, and the algorithm will compute an optimized control.\n",
    "This relieves some burden by letting the designer reason at the level of\n",
    "*what* the robot should do, rather than designing *how* it should do.\n",
    "\n",
    "In this chapter, we will discuss how to specify optimal control problems\n",
    "and how to implement and use optimal control techniques. We will\n",
    "consider only the open loop problem; however we will mention how an open\n",
    "loop optimizer can be adapted to closed loop control via the use of\n",
    "*model predictive control*.\n",
    "\n",
    "Optimal control problem\n",
    "-----------------------\n",
    "\n",
    "An optimal control problem is defined by the dynamics function $f$ and a\n",
    "*cost functional* over the entire trajectory $x$ and $u$:\n",
    "$$J(x,u) = \\int_0^\\infty L(x(t),u(t),t) dt.$$ The term *functional*\n",
    "indicates that this is a function mapping a function to a real number.\n",
    "The term $L(x,u,t)$ is known as the *instantaneous cost* (or *running cost*) which is\n",
    "accumulated over time, and should be chosen to be nonnegative and to\n",
    "penalize certain undesirable states, velocities, or controls. Its units\n",
    "are cost units per second.\n",
    "\n",
    "The goal of optimal control is to find state and control trajectories\n",
    "$x$ and $u$ such that $J(x,u)$ is minimized: $$\\begin{gathered}\n",
    "x^\\star, u^\\star = \\arg \\min_{x,u} J(x,u) \\text{ such that} \\\\\n",
    "\\dot{x}(t) = f(x(t),u(t)) \\text{ for all }t\n",
    "\\end{gathered}\n",
    "\\label{eq:OptimalControl}$$\n",
    "\n",
    "(For somewhat technical reasons, there are problems for which no optimal\n",
    "trajectory exists, but rather only a sequence of trajectories\n",
    "approaching an optimal cost. Hence, if we prefer to be pedantic, it is\n",
    "often necessary to prove existence of an optimal solution first, or to\n",
    "relax the problem to determine only an approximate optimal.)\n",
    "\n",
    "### Cost functionals\n",
    "\n",
    "A variety of behaviors can be specified in this framework by modifying\n",
    "the instantaneous cost. For example:\n",
    "\n",
    "- Trajectory tracking for a trajectory $x_D(t)$ can be implemented by penalizing squared error $L(x,u,t) = \\|x - x_D(t)\\|^2$.\n",
    "- Minimizing effort can be defined in terms of a control penalty $\\|u\\|^2$.\n",
    "- Minimum time to hit a target $x_{tgt}$ could be implemented as an indicator function $I[x\\neq x_{tgt}]$ where $I[z]$ is 1 if $z$ is true, and 0 otherwise.\n",
    "- Obstacle avoidance and other feasibility constraints can be implemented as indicator functions as well, $\\infty \\cdot I[x \\notin \\mathcal{F}]$ where $\\mathcal{F}$ is the free space.\n",
    "- Smoothed obstacle avoidance can be implemented by a repulsive barrier that decreases to 0 when the distance to the closest obstacle $d$ exceeds some minimum buffer distance $d_{min}$ and incrases to infinity as the distance shrinks to 0. One common form of this barrier is $L(x,u,t) = 1/d^2 - 1/d_{min}^2$ when $d < d_{min}$ and $L(x,u,t) = 0$ otherwise.\n",
    "\n",
    "It is common to mix and match different types of costs functionals using a _weighted cost functional_ $$J(x,u) = \\sum_{i=1}^N w_i J_i(x,u)$$\n",
    "where each $J_i(x,u)$ is some primitive cost functional and $w_i$ scales its contribution to the final cost. By tuning these weights, a designer can encourage the optimized trajectories to emphasize some aspects of the trajectory over others.\n",
    "\n",
    "\n",
    "### Finite horizon optimal control and discounting\n",
    "\n",
    "As stated the optimal control problem is somewhat ill-behaved because it\n",
    "involves an infinite integral, which could achieve infinite cost even\n",
    "for relatively well-behaved trajectories. For example, if the cost were\n",
    "simply squared error, a trajectory that achieves 0.01 steady-state error\n",
    "would be rated as having the same cost as a trajectory that had an error\n",
    "of 1: namely, infinitely bad.\n",
    "\n",
    "There are two general ways to make the cost functional better behaved.\n",
    "The first method to truncate the problem at some maximum time $T$,\n",
    "leading to a *finite-horizon optimal control* cost functional\n",
    "$$J(x,u) = \\int_0^T L(x(t),u(t),t) dt + \\Phi(x(T))$$ where $\\Phi(x)$ is\n",
    "a nonnegative *terminal cost* that penalizes the state attained at the\n",
    "terminal time.\n",
    "\n",
    "The second method is to modify the instantaneous cost functional by\n",
    "including a *discount factor* that decays to 0 relatively quickly as\n",
    "$t \\rightarrow \\infty$. This is usually expressed as the product of a\n",
    "time-independent term and a time-dependent discount factor term:\n",
    "$$L(x,u,t) = L(x,u,0) \\gamma(t)$$ with $\\gamma(t)$ a decaying function,\n",
    "such as $O(1/t^\\alpha)$ or $O(\\beta^t)$. It is important to choose a\n",
    "discount factor that drops relatively rapidly toward 0 to ensure that\n",
    "the cost is integrable. Discount factors of the form $O(1/t^\\alpha)$\n",
    "must have $\\alpha > 1$ to ensure that the cost functional is finite for\n",
    "all bounded trajectories, and those of the form $O(\\beta^t)$ must have\n",
    "$\\beta < 1$.\n",
    "\n",
    "### State and Control Constraints\n",
    "\n",
    "Usually, optimal control solvers require that the cost functional is\n",
    "smooth, and so non-differentiable constraints like minimum time and\n",
    "obstacle avoidance must be reformulated as hard constraints, external to\n",
    "the cost functional. As a result the reformulation becomes essentially\n",
    "an infinite-dimensional constrained optimization problem. Solvers may\n",
    "differ about whether they can handle constraints on state or constraints\n",
    "on control.\n",
    "\n",
    "### Analytical vs Numerical Solvers\n",
    "\n",
    "Minimization over a space of functions is considerably more difficult to\n",
    "solve than typical optimization problems: the space of functions is\n",
    "*uncountably infinite-dimensional*! There are two general ways to tackle\n",
    "these problems: analytical or numerical. Analytical techniques use the\n",
    "mathematical conditions of optimality so that the optimal control can be\n",
    "determined directly through calculus and algebraic manipulation.\n",
    "Successfully applying analysis typically requires a relatively simple\n",
    "dynamics and cost. Numerical techniques approximate the problem by\n",
    "discretizing either the state, time, and/or control space and attempt to cast\n",
    "the problem as a finite dimensional optimization. These are more\n",
    "general-purpose and can be applied to complex problems, but are more\n",
    "computationally expensive and usually require some parameter tuning to\n",
    "obtain high-quality solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LQR control\n",
    "-----------\n",
    "\n",
    "The simplest class of optimal control problems is LTI systems with costs\n",
    "that are quadratic in $x$ and $u$. Through the calculus of variations,\n",
    "which is beyond the scope of this book, the optimal control for this\n",
    "problem class can be determined analytically as a closed-form function\n",
    "of $x$.\n",
    "\n",
    "LTI systems with quadratic cost are specified as $\\dot{x} = Ax + Bu$ and\n",
    "$$L(x,u,t) = x^T Q x + u^T R u$$ where $Q$ and $R$ are symmetric\n",
    "matrices of size $n\\times n$ and $m\\times m$, respectively. The\n",
    "magnitude of entries of $Q$ penalize error from the equilibrium point,\n",
    "and the magnitude of entries of $R$ penalize control effort. The overall\n",
    "control functional is therefore\n",
    "$$J(x,u) = \\int_0^\\infty x(t)^T Q x(t) + u(t)^T R u(t).$$\n",
    "\n",
    "Here, the optimal control can be shown to be a linear function of $x$:\n",
    "$$u = -Kx$$ for the gain $K = R^{-1}B^T P$ defined as a function of an\n",
    "unknown matrix $P$. $P$ is a symmetric $n \\times n$ matrix that solves\n",
    "the following *Riccati equation* :\n",
    "$$A^TP + PA -  PBR^{-1}B^TP + Q = 0.$$ Numerical methods are available\n",
    "for solving the Riccati equation for $P$. This method is known as the\n",
    "Linear Quadratic Regulator (LQR) approach.\n",
    "\n",
    "As we showed in the [section on LTI stability](Control.ipynb#Stability-in-Linear-Time-Invariant-Systems),\n",
    "traditional pole placement methods can be\n",
    "used to derive a stable controller for LTI systems. However, the\n",
    "significance of the LQR controller compared to traditional pole\n",
    "stability analysis is that the performance metric is made explicit\n",
    "rather than implicit. Moreover, it gives a closed form solution for any\n",
    "dynamic model specified by $A,B$, so if more information is gathered\n",
    "that yields a better estimate for $A$ and $B$, the LQR method can be\n",
    "applied directly to obtain the optimal gains.\n",
    "\n",
    "Pointryagin's Minimum Principle\n",
    "-------------------------------\n",
    "\n",
    "The \"magic\" of the LQR solution is obtained through a more generic\n",
    "principle of optimal controllers called the *Pointryagin's minimum\n",
    "principle*. It defines a *first order*, *necessary* condition for a\n",
    "particular state/control trajectory to be an optimum. It is derived from\n",
    "($\\ref{eq:OptimalControl}$) via a combination of calculus of\n",
    "variations and the method of Lagrange multipliers. This will briefly be\n",
    "described here.\n",
    "\n",
    "In (equality) constrained optimization problems, the method of Lagrange\n",
    "multipliers defines an auxiliary Lagrange multiplier variable\n",
    "$\\lambda_i$ for each equality constraint. However, in optimal control\n",
    "problems, there are an infinite number of equality constraints\n",
    "$\\dot{x}(t) = f(x(t),u(t))$ defined for each point in time. As a result,\n",
    "the Lagrange multipliers for this problem are not single variables, but\n",
    "rather *trajectories* defined over time. This trajectory of multipliers\n",
    "is known as a *costate trajectory*\n",
    "$\\lambda(t):[0,\\infty)\\rightarrow \\mathbb{R}^n$.\n",
    "\n",
    "An auxiliary function, called the *Hamiltonian*, is defined over the\n",
    "system and costate at particular points in time:\n",
    "$$H(\\lambda,x,u,t)=\\lambda^T f(x,u) + L(x,u,t).$$ It is also possible to\n",
    "maintain control constraints $u\\in \\mathcal{U}\\subseteq \\mathbb{R}^m$.\n",
    "\n",
    "Pointryagin's minimum principle is then stated as follows. An optimal\n",
    "state/ control/ costate trajectory $(x^\\star,u^\\star,\\lambda^\\star)$\n",
    "satisfies for all $t \\geq 0$:\n",
    "\n",
    "1.  $H(\\lambda^\\star(t),x^\\star(t),u^\\star(t),t) \\leq H(\\lambda^\\star(t),x^\\star(t),u,t)$\n",
    "    for all $u \\in \\mathcal{U}$\n",
    "\n",
    "2.  $\\dot{x}^\\star(t) = f(x^\\star(t),u^\\star(t))$\n",
    "\n",
    "3.  $\\dot{\\lambda}^\\star(t) = -\\frac{\\partial}{\\partial x}H(\\lambda^\\star(t),x^\\star(t),u^\\star(t),t)$.\n",
    "\n",
    "The derivation of these equations is outside the scope of this book. But\n",
    "the conditions can be applied in certain cases to obtain optimal\n",
    "controls, or at least limit the range of controls possibly optimal.\n",
    "\n",
    "### Derivation of LQR from PMP\n",
    "\n",
    "As a result, consider the LQR setting. The Hamiltonian is\n",
    "$$H(\\lambda,x,u,t) = \\lambda^T(Ax + Bu) + x^T Q x + u^T R u$$ and the\n",
    "Pointryagin's minimum principle gives:\n",
    "\n",
    "1.  Subtracting off terms that do not contain $u$,\n",
    "    $\\lambda^{\\star T}Bu^{\\star} + u^{\\star T} R u^\\star \\leq \\lambda^{\\star T}Bu + u^T R u$\n",
    "    for all $u$.\n",
    "\n",
    "2.  $\\dot{x}^\\star = Ax^\\star + Bu^\\star$\n",
    "\n",
    "3.  $\\dot{\\lambda}^\\star = - A^T \\lambda^{\\star}  - 2 Qx$.\n",
    "\n",
    "Expanding 1, we see that for $u^\\star$ to be a minimizer of the\n",
    "Hamiltonian, given $\\lambda^\\star$, $x^\\star$, and $t$ fixed, we must\n",
    "have that $B^T \\lambda^\\star + 2 R u^\\star = 0$ so that\n",
    "$u^\\star = \\frac{1}{2}R^{-1} B^T \\lambda^\\star$.\n",
    "\n",
    "Now replacing this into 2 and 3, we have a system of ODEs:\n",
    "$$\\begin{split}\n",
    "\\dot{x}^\\star &= A x^\\star + \\frac{1}{2} B R^{-1} B^T \\lambda^\\star \\\\\n",
    "\\dot{\\lambda}^\\star &= - 2 Qx^\\star  -A^T \\lambda^\\star\n",
    "\\end{split}$$ \n",
    "\n",
    "Hypothesizing that $\\lambda^\\star = 2 P x^\\star$ and\n",
    "multiplying the first equation by $P$, we obtain the system of equations\n",
    "\n",
    "$$\\begin{split}\n",
    "P\\dot{x}^\\star &=  (PA + P B R^{-1} B^T P )x^\\star \\\\\n",
    "2P\\dot{x}^\\star &= (-2Q  -2A^T P)x^\\star.\n",
    "\\end{split}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dividing the second equation by 2 and equating the\n",
    "left hand sides, we have an equation that must be satisfied for all $x$.\n",
    "Since the equation must hold for all $x$, the matrices must also be\n",
    "equal, which produces the Riccati equations.\n",
    "\n",
    "### Bang-bang control\n",
    "\n",
    "Another result from Pointryagin's minimum principle condition (1) is\n",
    "that the Hamiltonian must be minimized by the control, keeping the state\n",
    "and costate fixed. As a result, there are two possibilities: (1a) the\n",
    "derivative of the Hamiltonian is 0 at $u^\\star$, or (1b) the control is\n",
    "at the boundary of the control set $u^\\star \\in \\partial U$.\n",
    "\n",
    "This leads to many systems having the characteristic of *bang-bang\n",
    "control*, which means that the optimal control will jump discontinuously\n",
    "between extremes of the control set. As an example, consider a race car\n",
    "driver attempting to minimize time. The optimal control at all points in\n",
    "time will either maximize acceleration, maximize braking, or\n",
    "maximize/minimize angular acceleration; otherwise, time could be saved\n",
    "by making the control more extreme.\n",
    "\n",
    "If it can be determined that there are a finite number of possible\n",
    "controls satisfying condition (1), then the optimal control problem\n",
    "becomes one of simply finding *switching times* between optimal\n",
    "controls.\n",
    "\n",
    "Taking the [Dubins car model](WhatAreDynamicsAndControl.ipynb#Dubins-car) as an example, \n",
    "we have the state variable\n",
    "$(x,y,\\theta)\\in SO(2)$ and control variable $(v,\\phi)$ denoting\n",
    "velocity and steering angle: $$\\begin{split}\n",
    "\\dot{x} &= v \\cos \\theta \\\\\n",
    "\\dot{y} &= v \\sin \\theta \\\\\n",
    "\\dot{\\theta} &= v/L \\tan \\phi .\n",
    "\\end{split}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $(x,y)$ are the coordinates of a point in the middle\n",
    "of the rear axis, and $L$ is the length between the rear and front axle.\n",
    "The velocity and steering angle are bounded, $v \\in [-1,1]$ and\n",
    "$\\phi \\in [-\\phi_{min},\\phi_{max}]$, and the cost only measures time to\n",
    "reach a target state. Hence, the Hamiltonian is\n",
    "$$H(\\lambda,x,u,t) = \\lambda_1 v \\cos \\theta + \\lambda_2 v \\sin \\theta + \\lambda_3 v/L \\tan \\phi + I[x \\neq x_{tgt}]$$\n",
    "\n",
    "The latter term does not contribute to the choice of $u$, so we can\n",
    "ignore it. For $(v,\\phi)$ to be a minimum of the Hamiltonian, with\n",
    "$\\lambda$ and $x$ fixed, either $\\lambda = 0$ and the control is\n",
    "irrelevant, or $\\lambda \\neq 0$ and\n",
    "$v = -sign(\\lambda_1 \\cos \\theta + \\lambda_2 \\sin \\theta + \\lambda_3 /L \\tan \\phi)$.\n",
    "Then, since $\\tan$ is a monotonic function, we have\n",
    "$\\phi = -sign(\\lambda_3 v)\\phi_{max}$. As a result, the only options are\n",
    "the minimum, maximum, and 0 controls on each axis.\n",
    "\n",
    "The trajectories corresponding to these extrema are straight\n",
    "forward/backward, moving forward while turning left/right, and moving\n",
    "backward while turning left/right. The curves traced out by these\n",
    "trajectories are then either straight line segments or arcs of turning\n",
    "rate $\\pm \\tan \\phi_{max}/L$. To find all minimum-time paths between two\n",
    "points, it is then a matter of enumerating all possible arcs and\n",
    "straight line segments. The solutions are known as Reeds-Shepp curves.\n",
    "\n",
    "Trajectory Optimization\n",
    "-----------------------\n",
    "\n",
    "It is not always possible (or easy) to derive elegant expressions of\n",
    "optimal controls using Pointryagin's minimum principle for general\n",
    "nonlinear systems. For example, if any modification is made to an LQR\n",
    "system, such as a non-quadratic term in the cost functional, or if state\n",
    "constraints or control constraints are added, the analytical solution no\n",
    "longer applies. As a result, numerical methods can solve a wider variety\n",
    "of optimal control problems. We have already seen\n",
    "a form of trajectory optimization under the discussion of kinematic path\n",
    "planning, and here we extend this type of formulation to dynamic systems.\n",
    "\n",
    "Since trajectories are infinite-dimensional, the main challenge of\n",
    "trajectory optimization is to suitably discretize the space of\n",
    "trajectories. A second challenge is that the discretized optimization\n",
    "problem is usually also fairly large, depending on the granularity of\n",
    "the discretization, and hence optimization may be computationally\n",
    "inefficient.\n",
    "\n",
    "In any case, the general method for performing trajectory optimization\n",
    "follows the following procedure 1) define a set of basis functions for the control\n",
    "trajectory, 2) defining a state evolution technique, 3) reformulating\n",
    "($\\ref{eq:OptimalControl}$) as a finite-dimensional optimization\n",
    "problem over the coefficients of the basis functions, and 4) optimizing\n",
    "using a gradient-based technique.\n",
    "\n",
    "### Piecewise-constant control and state trajectories\n",
    "\n",
    "The main choice in trajectory optimization is how to represent a\n",
    "trajectory of controls? The most typical assumption is that the\n",
    "trajectory consists of piecewise-constant controls at a fixed time step.\n",
    "\n",
    "If we define the time step $\\Delta t = T/N$ with $N$ an integer, then we\n",
    "have the *computational grid*\n",
    "$0, \\Delta t, 2\\Delta t, \\ldots, N\\Delta t$. Let these grid points be\n",
    "denoted $t_0,t_1,\\ldots,t_N$ respectively with $t_0=0$ and $t_N=T$.\n",
    "\n",
    "Then, the entire control trajectory is specified by a control sequence\n",
    "$u_1,\\ldots,u_N$, with each $u_i$ active on the time range\n",
    "$[t_{i-1},t_i)$. In other words $u(t) = u_i$ with\n",
    "$i = \\lfloor t/\\Delta t \\rfloor + 1$.\n",
    "\n",
    "Suppose now we define a *simulation function*, which is a method for\n",
    "integrating the state trajectory over time. Specifically, given an\n",
    "initial state $x(0)$, a constant control $u$, and a fixed duration $h$,\n",
    "the simulation function $g$ computes an approximation\n",
    "$$x(h) \\approx g(x(0),u,h)$$ If the timestep is small enough, the Euler\n",
    "approximation is a reasonable simulation function:\n",
    "$$x(h) \\approx x(0) +h f(x(0),u).$$ If accuracy of this method is too\n",
    "low, then Euler integration could be performed at a finer time sub-step\n",
    "up to time $h$, and/or a more accurate integration technique could be\n",
    "used.\n",
    "\n",
    "In any case, given a piecewise-constant control trajectory defined by a\n",
    "control sequence $u_1,\\ldots,u_N$, we can derive corresponding points on\n",
    "the state trajectory as follows.\n",
    "\n",
    "1.  Set $x_0 \\gets x(0)$.\n",
    "\n",
    "2.  For $i=1,\\ldots,N$, set $x_i = g(x_{i-1},u_i,\\Delta t)$ to arrive at a state sequence $x_0=x(0),x_1,\\ldots,x_N$. \n",
    "\n",
    "With this definition, each $x_i$ is a function of $u_1,\\ldots,u_i$. Hence, we can\n",
    "approximate the cost functional as:\n",
    "$$J(x,u) \\approx \\tilde{J}(u_1,\\ldots,u_n) = \\delta t \\sum_{i=0}^{N-1} L(x_i,u_{i+1},t_i)  + \\Phi(x_N).$$\n",
    "Using this definition we can express the approximated optimal control\n",
    "function as a minimization problem:\n",
    "$$\\arg \\min_{u_1,\\ldots,u_N} \\tilde{J}(u_1,\\ldots,u_N).\n",
    "\\label{eq:DirectTranscription}$$\n",
    "With control space $\\mathbb{R}^m$, this is an\n",
    "optimization problem over $mN$ variables.  This approach is also known\n",
    "as _direct transcription_.\n",
    "\n",
    "There is a tradeoff in determining the resolution $N$. With higher\n",
    "values of $N$, the control trajectory can obtain lower costs, but the\n",
    "optimization problem will have more variables, and hence become more\n",
    "computationally complex. Moreover, it will be more susceptible to local\n",
    "minimum problems.\n",
    "\n",
    "### Descent approaches\n",
    "\n",
    "Standard gradient-based techniques can be used to solve the problem\n",
    "($\\ref{eq:DirectTranscription}$). One difficulty is that to take the gradient\n",
    "of $\\tilde{J}$ with respect to a control variable $u_i$, observe that\n",
    "this choice of control affects *every* state variable $x_k$ with\n",
    "$i \\leq k \\leq N$. Hence,\n",
    "$$\\frac{\\partial J}{\\partial u_i} = \\Delta t \\frac{\\partial L}{\\partial u_i} (x_{i-1},u_i,t_i) + \\Delta t \\sum_{k=i}^{N-1} \\frac{\\partial L}{\\partial x}(x_k,u_{k+1},t_k)\\frac{\\partial x_k}{\\partial u_i} + \\frac{\\partial \\Phi}{\\partial x_N}\\frac{\\partial x_N}{\\partial u_i}\n",
    "\\label{eq:JacobianUi}$$ The expressions for\n",
    "$\\frac{\\partial x_k}{\\partial u_i}$ are relatively complex because $x_k$\n",
    "is defined recursively assuming that $x_{i-1}$ is known. $$\\begin{split}\n",
    "x_i &= g(x_{i-1},u_i,\\Delta t) \\\\\n",
    "x_{i+1} &= g(x_i,u_{i+1},\\Delta t) \\\\\n",
    "x_{i+2} &= g(x_{i+1},u_{i+2},\\Delta t) \\\\\n",
    "&\\vdots\n",
    "\\end{split}$$\n",
    "In this list, the only equation directly affected by $u_i$\n",
    "is the first. The effects on the remaining states are due to cascading\n",
    "effects of previous states. Hence, we see that\n",
    "$$\\frac{\\partial x_i}{\\partial u_i} = \\frac{\\partial g}{\\partial u}(x_{i-1},u_i,\\Delta t)$$\n",
    "$$\\frac{\\partial x_{i+1}}{\\partial u_i} = \\frac{\\partial x_{i+1}}{\\partial x_i}  \\frac{\\partial x_i}{\\partial u_i} = \\frac{\\partial g}{\\partial x}(x_i,u_{i+1},\\Delta t) \\frac{\\partial x_i}{\\partial u_i}$$\n",
    "And in general, for $k > i$,\n",
    "$$\\frac{\\partial x_k}{\\partial u_i} = \\frac{\\partial g}{\\partial x}(x_{k-1},u_k,\\Delta t) \\frac{\\partial x_{k-1}}{\\partial u_i}.$$\n",
    "This appears to be extremely computationally expensive, since each\n",
    "evaluation of ($\\ref{eq:JacobianUi}$) requires calculating $O(N)$ derivatives,\n",
    "leading to an overall $O(N^2)$ algorithm for calculating the gradient\n",
    "with respect to the entire control sequence.\n",
    "\n",
    "However, with a clever forward/backward formulation, the gradient can be calculated with $O(N)$ operations.\n",
    "Note that all expressions of the form\n",
    "$\\frac{\\partial x_k}{\\partial u_i}$ are equivalent to\n",
    "$\\frac{\\partial x_k}{\\partial x_i}\\frac{\\partial x_i}{\\partial u_i}$.\n",
    "So, we observe that\n",
    "($\\ref{eq:JacobianUi}$) is equal to\n",
    "$$\\Delta t \\frac{\\partial L}{\\partial u_i}(x_{i-1},u_i,t) + \\frac{\\partial J}{\\partial x_i} \\frac{\\partial x_i}{\\partial u_i}.$$\n",
    "Then, we can express:\n",
    "$$\\frac{\\partial J}{\\partial x_i} = \\Delta t \\sum_{k=i}^{N-1} \\frac{\\partial L}{\\partial x}(x_k,u_{k+1},t_k) \\frac{\\partial x_k}{\\partial x_i} + \\frac{\\partial \\Phi }{\\partial x_N}\\frac{\\partial x_N}{\\partial x_i}.$$\n",
    "This entire vector can be computed in a single backward pass starting\n",
    "from $i=N$ back to $i=1$. Starting with $i=N$, see that\n",
    "$$\\frac{\\partial J}{\\partial x_N} = \\frac{\\partial \\Phi }{\\partial x_N}$$\n",
    "Then, proceeding to $i=N-1$, observe $$\\begin{split}\n",
    "\\frac{\\partial J}{\\partial x_{N-1}} &= \\Delta t \\frac{\\partial L}{\\partial x}(x_{N-1},u_N,t_{N-1}) + \\frac{\\partial \\Phi }{\\partial x_N}\\frac{\\partial x_N}{\\partial x_{N-1}} \\\\\n",
    "&=  \\Delta t \\frac{\\partial L}{\\partial x}(x_{N-1},u_N,t_{N-1}) + \\frac{\\partial J}{\\partial x_N} \\frac{\\partial x_N}{\\partial x_{N-1}}.\n",
    "\\end{split}$$ In general, with $i<N$, we have the recursive expression\n",
    "$$\\frac{\\partial J}{\\partial x_i} =  \\Delta t \\frac{\\partial L}{\\partial x}(x_i,u_{i+1},t_i) + \\frac{\\partial J}{\\partial x_{i+1}} \\frac{\\partial x_{i+1}}{\\partial x_{i}}.$$\n",
    "The entire set of values can be computed in $O(N)$ time for all\n",
    "$x_1,\\ldots,x_N$.\n",
    "\n",
    "However, problems of this sort are usually poorly scaled, and hence\n",
    "standard gradient descent converges slowly. The most commonly used\n",
    "higher-order technique is known as _Differential Dynamic Programming_\n",
    "(DDP), which is an efficient recursive method for performing Newton's\n",
    "method. Given a current control trajectory, it approximates the cost\n",
    "function as a quadratic function of the controls, and solves for the\n",
    "minimum of the function.  A similar approach is the _Iterative LQR_ (iLQR) algorithm, which is very closely related to DDP but drops the 2nd derivative of the dynamics function. The exact steps for implementing DDP and iLQR are beyond\n",
    "the scope of this course but are readily available from other sources.\n",
    "\n",
    "### Pseudospectral / collocation methods\n",
    "\n",
    "As an alternative to piecewise constant controls, it is also possible to\n",
    "use other discretizations, such as polynomials or splines. In any case,\n",
    "the control is specified by a linear combination of *basis functions*\n",
    "$$u(t) = \\sum_{i=1}^k c_i \\beta_i(t)$$ where the $c_i \\in \\mathbb{R}^m$\n",
    "are control coefficients, which are to be chosen by the optimization,\n",
    "and the basis functions $\\beta_i(t)$ are constant. For example, a set of\n",
    "polynomial basis functions could be $1$, $t$, $t^2$, \\..., $t^{k-1}$.\n",
    "The difficulty with such parameterizations is that the state trajectory\n",
    "depends on every control coefficient, so evaluating the gradient is\n",
    "computationally expensive. \n",
    "\n",
    "To address this problem, it is typical to include a *state trajectory parameterization* in\n",
    "which the state trajectory $x(t)$ is also represented as an optimization variable that is\n",
    "parameterized explicitly along with the control trajectory. Specifically, we suppose that $$x(t) = \\sum_{i=1}^k d_i \\gamma_i(t)$$ where the $d_i \\in \\mathbb{R}^n$ are state coefficients to be optimized, and the basis functions $\\gamma_i$ are constant.  The main challenge is then to enforce dynamic consistency between the $x$ trajectory and the $u$ trajectory over the time domain.  Because it is impossible to do this exactly in a continuous infinity of points, the dynamic consistance must then be enforced at a finite number of points\n",
    "in time, which are known as *collocation points*. The result is an\n",
    "equality-constrained, finite-dimensional optimization problem.\n",
    "\n",
    "Specifically, given $N$ points in the time domain $t_1,\\ldots,t_N$, dynamic consistency is enforced at the $j$'th time point with a constraint\n",
    "$$\n",
    "x^\\prime(t_j) = f(x(t_j),u(t_j) )\n",
    "$$\n",
    "which can be rewritten in terms of the cofficients\n",
    "$$\n",
    "\\sum_{i=1}^k d_i \\gamma_i^\\prime(t_j) = f\\left(\\sum_{i=1}^k d_i \\gamma_i(t_j), \\sum_{i=1}^k c_i \\beta_i(t_j) \\right).\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Handling infinite-horizon problems\n",
    "\n",
    "There are some challenges when applying trajectory optimization to\n",
    "infinite-horizon optimal control problems. Specifically, it is not\n",
    "possible to define a computational grid over the infinite domain\n",
    "$[0,\\infty)$ for the purposes of computing the integral in $J(x,u)$. To\n",
    "do so, there are two general techniques available. The first is to\n",
    "simply truncate the problem at some maximum time $T$, leading to a\n",
    "finite-horizon optimal control problem.\n",
    "\n",
    "The second method is to reparameterize time so that the range\n",
    "$[0,\\infty)$ is transformed into a finite range, say $[0,1]$. If we let\n",
    "$s=1-e^t$ then $s$ is in the range $[0,1]$. The cost functional then\n",
    "becomes:\n",
    "$$J(x,u) = \\int_0^1 L(x(-\\ln(1-s)),u(-\\ln(1-s)),-\\ln(1-s)) / (1-s) ds.$$\n",
    "This leads to a finite-horizon optimal control problem over the $s$\n",
    "domain, with $T=1$. Hence, if a uniform grid is defined over\n",
    "$s \\in [0,1]$, then the grid spacing in the time domain becomes\n",
    "progressively large as $t$ increases.\n",
    "\n",
    "In the reformulated problem it is necessary to express the derivative of\n",
    "$x$ with respect to $s$ in the new dynamics:\n",
    "$$\\frac{d}{d s} x(t(s)) = \\dot{x}(t(s)) t^\\prime(s) = f(x(t(s)),u(t(s))) / (1-s)$$\n",
    "Care must also be taken as $s$ approaches 1, since the $1/(1-s)$ term\n",
    "approaches infinity, and if instantaneous cost does not also approach 0,\n",
    "then cost will become infinite. It is therefore customary to use a\n",
    "discount factor. With an appropriately defined discount term, the $s=1$\n",
    "contribution to cost will be dropped.\n",
    "\n",
    "### Local minima\n",
    "\n",
    "A major issue with any descent-based trajectory optimization approach is\n",
    "local minima. Only in a few cases can we prove that the problem is\n",
    "convex, such as in LTI problems with convex costs and linear control\n",
    "constraints. As we have seen before, random restarts are one of the most\n",
    "effective ways to handle local minima, but in the high dimensional spaces of\n",
    "trajectory optimization, a prohibitive number of restarts is needed to\n",
    "have a high chance of finding a global optimum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hamilton-Jacobi-Bellman Equation\n",
    "--------------------------------\n",
    "\n",
    "An alternative method to solve optimal control problems is to find the\n",
    "solution in *state space* rather than the time domain. In the\n",
    "Hamilton-Jacobi-Bellman (HJB) equation, so named as an extension of the\n",
    "Bellman equation for discrete optimal planning problems, a partial\n",
    "differential equation (PDE) across state space is formulated to\n",
    "determine the optimal control everywhere. (Contrast this with the\n",
    "Pointryagin's minimum principle, which is an optimality condition only\n",
    "along a single trajectory.)\n",
    "\n",
    "### Derivation\n",
    "\n",
    "We start by formulating the HJB equation in discrete time. Consider a\n",
    "finite-horizon optimal control problem, and define the *value function*\n",
    "as a function\n",
    "$V(x,t) : \\mathbb{R}^n \\times \\mathbb{R} \\rightarrow \\mathbb{R}$ that defines the\n",
    "*minimum possible accumulated cost* that could be obtained by any trajectory\n",
    "starting from initial state $x$ and time $t$.  In other words, let us define the\n",
    "truncated cost functional\n",
    "$$J_t(x,u) = \\int_t^T L(x,u,s) ds + \\Phi(x(T))$$\n",
    "which truncates the lower point in the integral term of $J(x,u)$ to start from time\n",
    "$t$. (Obviously, $J(x,u)=J_0(x,u)$.)  Then, the value function is the minimizer of\n",
    "the truncated cost over all possible future controls: $V(x,t) = \\min_u(J_t(x,u))$.\n",
    "\n",
    "It is apparent that at time $T$, the only term that remains is the\n",
    "terminal cost, so one boundary term is given: $$V(x,T) = \\Phi(x).$$ Now\n",
    "we examine the value function going backwards in time. Suppose we know\n",
    "$V(x,t+\\Delta t)$ for all $x$, and now we are considering time $t$. Let\n",
    "us also assume that at a state $x$ with control $u$, the resulting state\n",
    "at time $T$ is approximated by Euler integration, and the incremental\n",
    "cost is approximately constant over the interval $[t,t+\\Delta t)$. Then,\n",
    "we have the approximation\n",
    "$$V(x,t) \\approx \\min_{u\\in U} [ \\Delta t L(x,u,t) + V(x + \\Delta t f(x,u),t+\\Delta t)]\n",
    "\\label{eq:DiscreteTimeHJB}$$ The minimization is taken over controls to\n",
    "find the optimal control for the next time step. The first term of the\n",
    "minimized term includes the incremental cost from the current state,\n",
    "time, and chosen control. The second term includes the cost contribution\n",
    "from the next state under the chosen control, incremented forward in\n",
    "time.\n",
    "\n",
    "Note that the first order approximation of $V(x,t)$ is given by:\n",
    "$$V(x+\\Delta x,t+\\Delta t) \\approx V(x,t) + \\frac{\\partial V}{\\partial x}(x,t) \\Delta x + \\dot{V}(x,t)\\Delta t$$\n",
    "If we take the limit of\n",
    "($\\ref{eq:DiscreteTimeHJB}$) as the time step $\\Delta t$ approaches\n",
    "0, subtract $V(x,t)$ from both sides, and divide by $\\Delta t$, then we\n",
    "obtain the Hamilton-Jacobi-Bellman PDE :\n",
    "$$0 = \\dot{V}(x,t) + \\min_{u \\in U} [ L(x,u,t) + \\frac{\\partial V}{\\partial x}(x,t) f(x,u)].\n",
    "\\label{eq:HJB}$$\n",
    "\n",
    "If these equations were to be solved either in discrete or continuous\n",
    "time across the $\\mathbb{R}^n \\times \\mathbb{R}$ state space, then we\n",
    "have a complete description of optimal cost starting from any state. It\n",
    "is also possible to enforce state constraints simply by setting the\n",
    "value function at inadmissible states to $\\infty$. Moreover, it is a\n",
    "relatively straightforward process to determine the optimal control\n",
    "given a value function:\n",
    "$$u^\\star(x,t) = \\arg \\min_{u \\in U} [ \\Delta t L(x,u,t) + V(x + \\Delta t f(x,u),t + \\Delta t)]$$\n",
    "for the discrete case and\n",
    "$$u^\\star(x,t) = \\arg \\min_{u \\in U} [ L(x,u,t) + \\frac{\\partial V}{\\partial x}(x,t) f(x,u)]$$\n",
    "for the continuous case. The main challenge here is to represent and\n",
    "calculate a function over an $n+1$ dimensional grid, which is\n",
    "prohibitively expensive for high-D state spaces. It is also potentially\n",
    "difficult to perform the minimization over the control\n",
    "in ($\\ref{eq:DiscreteTimeHJB}$)\n",
    "and ($\\ref{eq:HJB}$),\n",
    "since it must be performed at each point in time and space.\n",
    "\n",
    "### Reducing dimension by 1 using time-independence\n",
    "\n",
    "It is often useful to reduce the dimensionality down to an $n$-D grid if\n",
    "the incremental cost is time-independent and the problem has an infinite\n",
    "horizon. With these assumptions, the optimal control is *stationary*,\n",
    "that is, it is dependent only on state and not time. Then, we can set up\n",
    "a set of recursive equations on a time-independent value function:\n",
    "$$V(x) = \\min_{u \\in U} [ \\Delta L(x,u) + V(x+\\Delta t f(x,u)) ]\n",
    "\\label{eq:DiscreteHJBStationary}$$ in the discrete time case, or taking\n",
    "the limit as $\\Delta t \\rightarrow 0$, we get the continuous PDE\n",
    "$$0 = \\min_{u \\in U} [ L(x,u) + \\frac{\\partial V}{\\partial x}(x) f(x,u) ].$$\n",
    "It can be rather challenging to solve these equations exactly due to\n",
    "their recursive nature. Also, some discretization of the control set $U$\n",
    "is usually needed, and a finer discretization will help the method\n",
    "compute better estimates. Three general methods exist for doing so.\n",
    "\n",
    "1.  Value iteration uses a guess of $V(x)$ and then iteratively improves\n",
    "    it by optimizing\n",
    "    ($\\ref{eq:DiscreteHJBStationary}$) on each $x$ in the grid. This\n",
    "    is also known as recursive dynamic programming.\n",
    "\n",
    "2.  Linear programming uses a set of sample points $x_1,\\ldots,x_N$ on a\n",
    "    state space grid and points $u_1,\\ldots,u_M$ on a control space\n",
    "    grid, and then sets up a large linear programming problem with\n",
    "    constraints of the form\n",
    "    ($\\ref{eq:DiscreteHJBStationary}$).\n",
    "\n",
    "3.  Policy iteration assigns guesses for the policy $u(x)$, and\n",
    "    iteratively alternates between a) solving for the $V(x)$ induced by\n",
    "    those controls, and b) improving the assigned controls using the\n",
    "    induced $V(x)$.\n",
    "\n",
    "Model Predictive Control\n",
    "------------------------\n",
    "\n",
    "The method of model predictive control (MPC) is a process for building a\n",
    "closed-loop controller when given a method that computes open loop\n",
    "trajectories. Generally speaking, it simply replans a new trajectory\n",
    "starting from the sensed state at each step. It executes some small\n",
    "portion of that trajectory, senses the new state, and then replans\n",
    "again. By repeating this process, MPC is able to cope with unexpected\n",
    "disturbances by dynamically calculating paths to return to desirable\n",
    "states.\n",
    "\n",
    "There are, however, several caveats involved in successful application\n",
    "of MPC. Let us define the steps more specifically. For a control loop\n",
    "that operates at rate $\\Delta t$, perform the following steps:\n",
    "\n",
    "1.  Sense the current state $x_c$\n",
    "\n",
    "2.  Compute a finite-time optimal trajectory $x,u$ starting at\n",
    "    $x(0) = x_c$.\n",
    "\n",
    "3.  Execute the control $u(t)$ for $t \\in [0,\\Delta t)$\n",
    "\n",
    "4.  Repeat from step 1.\n",
    "\n",
    "There are several variables of note when creating an MPC approach.\n",
    "First, the time step $\\Delta t$ must be long enough for step 2 to find\n",
    "an optimal trajectory. Second, the time horizon used in step 2 is an\n",
    "important variable, because it should be long enough for MPC to benefit\n",
    "from predictive lookahead, but not too long to make computational time\n",
    "exceed $\\Delta t$. Third, when recasting the problem as a finite-time\n",
    "optimal control problem, the terminal cost should be somewhat\n",
    "approximate the problem's value function, or else the system may not\n",
    "converge properly. Finally, the optimization method used ought to be\n",
    "extremely reliable. Failures in step 2 can be tolerated to some extent\n",
    "simply by using the previous trajectory segment, but to achieve good\n",
    "performance Step 2 should succeed regularly.\n",
    "\n",
    "Due to all of these variables, MPC is difficult to analyze, and as\n",
    "employed in practice it usually does not satisfy many theoretical\n",
    "stability / convergence properties. However, with careful tuning, MPC\n",
    "can an extremely high performing and practical nonlinear optimal control\n",
    "technique.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "-------\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "-   Optimal control functions are specified by a dynamics function, a\n",
    "    cost functional, and optional constraints. By changing the cost\n",
    "    functional, different performance objectives can be specified.\n",
    "\n",
    "-   Analytical optimal control techniques can be challenging to apply to\n",
    "    a given problem, but for some problems can yield computationally\n",
    "    efficient solutions by greatly reducing the search space.\n",
    "\n",
    "-   Numerical optimal control techniques are more general, but require\n",
    "    more computation.\n",
    "\n",
    "-   In trajectory optimization, time is discretized to produce a\n",
    "    finite-dimensional optimization problem. There is a tradeoff between\n",
    "    optimality and speed in the choice of computational grid resolution,\n",
    "    and are also susceptible to local minima.\n",
    "\n",
    "-   In Hamilton-Jacobi-Bellman (HJB) techniques, both time and space are\n",
    "    discretized. There are no local minima, but the technique suffers\n",
    "    from the curse of dimensionality.\n",
    "\n",
    "-   Model predictive control (MPC) turns open-loop trajectory\n",
    "    optimization into a closed-loop controller by means of repeated\n",
    "    replanning.\n",
    "\n",
    "The [following table](#tab:OptControlSummary) lists an overview of the approaches\n",
    "covered in this chapter.\n",
    "\n",
    "********************************************************************\n",
    "\n",
    "<div class=\"figcaption\"><a name=\"tab:OptControlSummary\">Summary of optimal control approaches</a></div>\n",
    "\n",
    "| **Approach**     | **Type**   | **Characteristics** |\n",
    ":------------------:|:------------:|:-----------------------------------------------------------------------:\n",
    "| LQR              |Analytical  |Applies to LTI systems with quadratic costs |\n",
    "| PMP              |Analytical  |Defines necessary conditions for optimality |\n",
    "| Trajectory opt.  |Numerical   |Optimize a time-discretized control or trajectory space. Local minima |\n",
    "| HJB              |Numerical   |Discretize and solve over state space |\n",
    "| MPC              |Numerical   |Closed-loop control by repeated optimization |\n",
    "\n",
    "********************************************************************\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises\n",
    "---------\n",
    "\n",
    "1.  Consider devising an optimal control formulation that describes how\n",
    "    your arm should reach for a cup. What is the state $x$? The control\n",
    "    $u$? The dynamics $f(x,u)$? The objective functional? Is an\n",
    "    infinite-horizon or finite-horizon more appropriate for this\n",
    "    problem? Do the same for the task of balancing on one foot.\n",
    "\n",
    "2.  Recall the point mass double-integrator system:\n",
    "    $$\\dot{x} \\equiv \\begin{bmatrix}{\\dot{p}}\\\\{\\dot{v}}\\end{bmatrix} = f(x,u) = \\begin{bmatrix}{v}\\\\{u/M}\\end{bmatrix}.$$\n",
    "    Express this as an LTI system, and solve for the LQR gain matrix $K$\n",
    "    with the cost terms $Q=\\begin{bmatrix}{10}&{0}\\\\{0}&{1}\\end{bmatrix}$ and $R=5$.\n",
    "\n",
    "3. Let $V^*(x)$ denote the infinite-horizon value function (i.e., the cost incurred by the optimal infinite-horizon control starting at $x$).  Now define a finite-horizon optimal control problem with horizon $T$, the same incremental cost $L$, and terminal cost $\\Phi(x) = V^*(x)$. Prove that the solution to this finite-horizon optimal control problem is identical to the infinite horizon optimal control problem for all $x$.\n",
    "\n",
    "4. Let $V^*_T(x)$ denote the $T$-horizon value function for a terminal cost function $\\Phi$.  Suppose that $0 \\leq T_1 < T_2$. Is it always true that $V^*_{T2}(x) \\geq V^*_{T1}(x)$?  If so, give a proof. If not, define a condition on $\\Phi$ that would make this condition true.\n",
    "\n",
    "5.  TBD\\...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": true,
   "report_style_numbering": false,
   "user_envs_cfg": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
